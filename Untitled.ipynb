{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1502bbb6-3d24-4546-8b06-4753beea255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27ff4575-5d89-49e2-b536-5216acd24633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class ItalianLyricsDataset(Dataset):\n",
    "    def __init__(self, lyrics, tokenizer, max_length):\n",
    "        self.lyrics = lyrics\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lyrics)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lyric = self.lyrics[idx]\n",
    "        encoding = self.tokenizer(lyric, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        return encoding['input_ids'].squeeze(), encoding['attention_mask'].squeeze()\n",
    "\n",
    "# RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        return self.fc(output)\n",
    "\n",
    "# LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.lstm(embedded)\n",
    "        return self.fc(output)\n",
    "\n",
    "# Function to train models\n",
    "def train_model(model, dataloader, criterion, optimizer, device, epochs, is_transformer=False):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            inputs, masks = batch\n",
    "            inputs, masks = inputs.to(device), masks.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if is_transformer:\n",
    "                outputs = model(inputs, attention_mask=masks, labels=inputs)\n",
    "                loss = outputs.loss\n",
    "            else:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), inputs.view(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Custom generation function for RNN and LSTM models\n",
    "def generate_custom(model, tokenizer, seed_text, max_length=50):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    input_ids = tokenizer.encode(seed_text, return_tensors='pt').to(device)\n",
    "    generated = input_ids[0].tolist()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - len(generated)):\n",
    "            inputs = torch.tensor([generated]).to(device)\n",
    "            outputs = model(inputs)\n",
    "            next_token_logits = outputs[0, -1, :]\n",
    "            next_token = torch.multinomial(torch.softmax(next_token_logits, dim=-1), num_samples=1).item()\n",
    "            generated.append(next_token)\n",
    "            if next_token == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True)\n",
    "\n",
    "# Generation function for transformer model\n",
    "def generate_transformer(model, tokenizer, seed_text, max_length=50):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(seed_text, return_tensors='pt').to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, temperature=0.7)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f764c2f-8521-4e54-92f0-70c97b49720f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50257. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN model...\n",
      "Epoch 1/3, Loss: 5.3852\n",
      "Epoch 2/3, Loss: 1.4757\n",
      "Epoch 3/3, Loss: 0.5397\n",
      "RNN training took 5.04 minutes\n",
      "Training LSTM model...\n",
      "Epoch 1/3, Loss: 6.6381\n",
      "Epoch 2/3, Loss: 3.2004\n",
      "Epoch 3/3, Loss: 1.5834\n",
      "LSTM training took 5.26 minutes\n",
      "Training Transformer model...\n",
      "Epoch 1/3, Loss: 5.6811\n",
      "Epoch 2/3, Loss: 3.7114\n",
      "Epoch 3/3, Loss: 3.2860\n",
      "Transformer training took 102.26 minutes\n",
      "\n",
      "Generating sample lyrics:\n",
      "RNN: Laura sei la più� cavaliusiusiusiusiusiusemaamamamamamamamamamamamamamamamamamamamamamamamamamamamamamamamamamam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM: Laura sei la più��� david pens pens pens pens pens pens pens pens pens pens pens pens pens pens pens pens pens pens pens pens pens pens saogl rend putus folk quiverver turb abl st st st st st\n",
      "Transformer: Laura sei la più lontano essere più lontano essere più lontano essere più lontano essere più lontano essere pi\n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    with open('./data/italian_lyrics.txt', 'r', encoding='utf-8') as f:\n",
    "        lyrics = f.readlines()\n",
    "\n",
    "    # Use a subset of the data for faster training\n",
    "    num_songs = 1000  # Adjust this number as needed\n",
    "    lyrics = lyrics[:num_songs]\n",
    "    \n",
    "    # Initialize tokenizer and models\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    vocab_size = tokenizer.vocab_size  #len(tokenizer)\n",
    "    embedding_dim = 256\n",
    "    hidden_dim = 512\n",
    "    max_length = 128\n",
    "    batch_size = 32\n",
    "    epochs = 3\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    device = torch.device('cpu')  # Explicitly use CPU\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = ItalianLyricsDataset(lyrics, tokenizer, max_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Initialize models\n",
    "    rnn_model = RNNModel(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "    lstm_model = LSTMModel(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "    transformer_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "    transformer_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Train models\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    rnn_optimizer = optim.Adam(rnn_model.parameters())\n",
    "    lstm_optimizer = optim.Adam(lstm_model.parameters())\n",
    "    transformer_optimizer = optim.Adam(transformer_model.parameters())\n",
    "\n",
    "    # print(\"Training RNN model...\")\n",
    "    # train_model(rnn_model, dataloader, criterion, rnn_optimizer, device, epochs)\n",
    "\n",
    "    # print(\"Training LSTM model...\")\n",
    "    # train_model(lstm_model, dataloader, criterion, lstm_optimizer, device, epochs)\n",
    "\n",
    "    # print(\"Training Transformer model...\")\n",
    "    # train_model(transformer_model, dataloader, criterion, transformer_optimizer, device, epochs, is_transformer=True)\n",
    "    models = [\n",
    "        (\"RNN\", rnn_model, rnn_optimizer),\n",
    "        (\"LSTM\", lstm_model, lstm_optimizer),\n",
    "        (\"Transformer\", transformer_model, transformer_optimizer)\n",
    "    ]\n",
    "\n",
    "    for model_name, model, optimizer in models:\n",
    "        print(f\"Training {model_name} model...\")\n",
    "        start_time = time.time()\n",
    "        train_model(model, dataloader, criterion, optimizer, device, epochs, \n",
    "                    is_transformer=(model_name == \"Transformer\"))\n",
    "        end_time = time.time()\n",
    "        print(f\"{model_name} training took {(end_time - start_time) / 60:.2f} minutes\")\n",
    "\n",
    "    # Generate sample lyrics\n",
    "    print(\"\\nGenerating sample lyrics:\")\n",
    "    seed_text = \"Laura sei la più\"\n",
    "    print(\"RNN:\", generate_custom(rnn_model, tokenizer, seed_text))\n",
    "    print(\"LSTM:\", generate_custom(lstm_model, tokenizer, seed_text))\n",
    "    print(\"Transformer:\", generate_transformer(transformer_model, tokenizer, seed_text))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1eed63b-b3b8-427b-9070-71f65697f343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50257. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN model...\n",
      "Epoch 1/3, Loss: 0.9600\n",
      "Epoch 2/3, Loss: 0.0404\n",
      "Epoch 3/3, Loss: 0.0098\n",
      "RNN training took 153.38 minutes\n",
      "Training LSTM model...\n",
      "Epoch 1/3, Loss: 1.6617\n",
      "Epoch 2/3, Loss: 0.1141\n",
      "Epoch 3/3, Loss: 0.0416\n",
      "LSTM training took 240.32 minutes\n",
      "Training Transformer model...\n",
      "Epoch 1/3, Loss: 3.5817\n",
      "Epoch 2/3, Loss: 2.9421\n",
      "Epoch 3/3, Loss: 2.7812\n",
      "Transformer training took 992.20 minutes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and preprocess data\n",
    "with open('./data/italian_lyrics.txt', 'r', encoding='utf-8') as f:\n",
    "    lyrics = f.readlines()\n",
    "\n",
    "# Use a subset of the data for faster training\n",
    "# num_songs = 1000  # Adjust this number as needed\n",
    "# lyrics = lyrics[:num_songs]\n",
    "\n",
    "# Initialize tokenizer and models\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "vocab_size = tokenizer.vocab_size  #len(tokenizer)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "max_length = 128\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')  # Explicitly use CPU\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = ItalianLyricsDataset(lyrics, tokenizer, max_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize models\n",
    "rnn_model = RNNModel(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "lstm_model = LSTMModel(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "transformer_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "transformer_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Train models\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "rnn_optimizer = optim.Adam(rnn_model.parameters())\n",
    "lstm_optimizer = optim.Adam(lstm_model.parameters())\n",
    "transformer_optimizer = optim.Adam(transformer_model.parameters())\n",
    "\n",
    "models = [\n",
    "    (\"RNN\", rnn_model, rnn_optimizer),\n",
    "    (\"LSTM\", lstm_model, lstm_optimizer),\n",
    "    (\"Transformer\", transformer_model, transformer_optimizer)\n",
    "]\n",
    "\n",
    "for model_name, model, optimizer in models:\n",
    "    print(f\"Training {model_name} model...\")\n",
    "    start_time = time.time()\n",
    "    train_model(model, dataloader, criterion, optimizer, device, epochs, \n",
    "                is_transformer=(model_name == \"Transformer\"))\n",
    "    end_time = time.time()\n",
    "    print(f\"{model_name} training took {(end_time - start_time) / 60:.2f} minutes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "381ca73e-b972-4fda-9b77-c2f1bb7412da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating sample lyrics:\n",
      "RNN: Stamattina il sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM: Stamattina il sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole sole\n",
      "Transformer: Stamattina il sole volere volere volere volere volere volere volere volere volere volere volere volere volere volere vole\n"
     ]
    }
   ],
   "source": [
    "# Generate sample lyrics\n",
    "print(\"\\nGenerating sample lyrics:\")\n",
    "seed_text = \"Stamattina il sole\"\n",
    "print(\"RNN:\", generate_custom(rnn_model, tokenizer, seed_text))\n",
    "print(\"LSTM:\", generate_custom(lstm_model, tokenizer, seed_text))\n",
    "print(\"Transformer:\", generate_transformer(transformer_model, tokenizer, seed_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76667a39-7833-4f8e-93f4-48c84fdda190",
   "metadata": {},
   "source": [
    "The repetitive output you're seeing is a common issue in language models, often referred to as \"repetition collapse\" or \"degeneration\". This usually happens when the model hasn't learned enough variety or when the sampling strategy isn't diverse enough.\n",
    "\n",
    "Key changes in the updated script below:\n",
    "\n",
    "We've implemented nucleus sampling (top-k and top-p) for the RNN and LSTM models in the generate_custom function.\n",
    "For the transformer model, we've added top-k, top-p, and repetition penalty parameters in the generate_transformer function.\n",
    "We've added gradient clipping in the train_model function to prevent exploding gradients.\n",
    "We've adjusted the learning rates for each model type.\n",
    "We're now passing the attention mask to the RNN and LSTM models during training.\n",
    "\n",
    "These changes should help reduce the repetitive outputs and improve the quality of the generated lyrics. Here's a brief explanation of the new parameters:\n",
    "\n",
    "temperature: Controls the randomness of predictions. Lower values make the model more confident but also more repetitive.\n",
    "top_k: Limits the sampling to the k most likely next words.\n",
    "top_p (nucleus sampling): Limits the sampling to the smallest set of words whose cumulative probability exceeds p.\n",
    "repetition_penalty: Penalizes repetitions in the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe3b9a49-34e3-4e4d-9bdc-ec1a36b0aee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated custom generation function for RNN and LSTM models\n",
    "def generate_custom(model, tokenizer, seed_text, max_length=50, temperature=0.7):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    input_ids = tokenizer.encode(seed_text, return_tensors='pt').to(device)\n",
    "    attention_mask = torch.ones_like(input_ids).to(device)\n",
    "    generated = input_ids[0].tolist()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - len(generated)):\n",
    "            inputs = torch.tensor([generated]).to(device)\n",
    "            masks = attention_mask[:, :inputs.shape[1]]\n",
    "            outputs = model(inputs, attention_mask=masks)\n",
    "            next_token_logits = outputs[0, -1, :] / temperature\n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=0.9)\n",
    "            next_token = torch.multinomial(torch.softmax(filtered_logits, dim=-1), num_samples=1).item()\n",
    "            generated.append(next_token)\n",
    "            attention_mask = torch.cat([attention_mask, torch.ones((1, 1)).to(device)], dim=1)\n",
    "            if next_token == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True)\n",
    "\n",
    "# Updated generation function for transformer model\n",
    "def generate_transformer(model, tokenizer, seed_text, max_length=50):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(seed_text, return_tensors='pt').to(model.device)\n",
    "    attention_mask = torch.ones_like(input_ids).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids, \n",
    "            max_length=max_length, \n",
    "            num_return_sequences=1, \n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            attention_mask=attention_mask,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Helper function for nucleus sampling\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    top_k = min(top_k, logits.size(-1))\n",
    "    if top_k > 0:\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "# Updated train_model function\n",
    "def train_model(model, dataloader, criterion, optimizer, device, epochs, is_transformer=False):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            inputs, masks = batch\n",
    "            inputs, masks = inputs.to(device), masks.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if is_transformer:\n",
    "                outputs = model(inputs, attention_mask=masks, labels=inputs)\n",
    "                loss = outputs.loss\n",
    "            else:\n",
    "                outputs = model(inputs, attention_mask=masks)\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), inputs.view(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03392a75-129f-47b3-864e-6b5da0967ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50257. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN model...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'attention_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 43\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_transformer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTransformer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m training took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(end_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 74\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, criterion, optimizer, device, epochs, is_transformer)\u001b[0m\n\u001b[1;32m     72\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), inputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     77\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/airisk/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'attention_mask'"
     ]
    }
   ],
   "source": [
    "# Use a subset of the data for faster training\n",
    "num_songs = 10  # Adjust this number as needed\n",
    "lyrics = lyrics[:num_songs]\n",
    "\n",
    "# Initialize tokenizer and models\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "vocab_size = tokenizer.vocab_size  #len(tokenizer)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "max_length = 128\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')  # Explicitly use CPU\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = ItalianLyricsDataset(lyrics, tokenizer, max_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize models\n",
    "rnn_model = RNNModel(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "lstm_model = LSTMModel(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "transformer_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "transformer_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Train models\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "rnn_optimizer = optim.Adam(rnn_model.parameters(), lr=0.001)\n",
    "lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "transformer_optimizer = optim.Adam(transformer_model.parameters(), lr=5e-5)\n",
    "\n",
    "models = [\n",
    "    (\"RNN\", rnn_model, rnn_optimizer),\n",
    "    (\"LSTM\", lstm_model, lstm_optimizer),\n",
    "    (\"Transformer\", transformer_model, transformer_optimizer)\n",
    "]\n",
    "\n",
    "for model_name, model, optimizer in models:\n",
    "    print(f\"Training {model_name} model...\")\n",
    "    start_time = time.time()\n",
    "    train_model(model, dataloader, criterion, optimizer, device, epochs, \n",
    "                is_transformer=(model_name == \"Transformer\"))\n",
    "    end_time = time.time()\n",
    "    print(f\"{model_name} training took {(end_time - start_time) / 60:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26752732-3541-43cd-9029-53a334e0d944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
